{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment #5: Extraction of subject–verb–object triples\n",
    "Author: Pierre Nugues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will extract relations from a parsed sentence involving two words or entities. You will start with pairs of words, namely a subject and its verb, and then extend your programs to triples: subject, verb, and object. In the triples, the subject and the object are the entities, and the verb represents the relation. \n",
    "\n",
    "$$\n",
    "\\text{Subject} \\xrightarrow[\\text{}]{\\text{Verb}} \\text{Object}\n",
    "$$\n",
    "\n",
    "The overall work is inspired by the _Prismatic_ knowledge base used in the IBM Watson system, where the subject, verb, and object triples are a way to extract knowledge from text.  See <a href=\"http://www.aclweb.org/anthology/W/W10/W10-0915.pdf\">this paper</a> for details. \n",
    "\n",
    "You will apply the extraction to multilingual texts: \n",
    "1. First you will use a parsed corpus of Swedish; and then\n",
    "2. You will apply it to other languages.\n",
    "            \n",
    "The objectives of this assignment are to:\n",
    "* Extract the subject–verb pairs from a parsed corpus\n",
    "* Extend the extraction to subject–verb–object triples\n",
    "* Understand how dependency parsing can help create a knowledge base\n",
    "* Write a short report of 1 to 2 pages on the assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As corpora, you will use the Universal Dependencies: https://universaldependencies.org/.\n",
    "1. In the first part of the assignment, you will focus on Swedish as it is easier to understand for most students, and then \n",
    "2. Move on to all the other languages. \n",
    "\n",
    "You will only consider the training sets of each corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a parsed corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "1. Download manually the latest version of Universal dependencies (2.8.1); see here `https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-3687`;\n",
    "2. Alternatively, you can run the command below, preferably from a terminal window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
      "  0  410M    0 15684    0     0   7688      0 15:33:26  0:00:02 15:33:24  7703\n",
      "  0  410M    0 1839k    0     0   624k      0  0:11:13  0:00:02  0:11:11  625k\n",
      "  1  410M    1 4543k    0     0  1148k      0  0:06:06  0:00:03  0:06:03 1149k\n",
      "  2  410M    2 9359k    0     0  1892k      0  0:03:42  0:00:04  0:03:38 1893k\n",
      "  3  410M    3 13.8M    0     0  2382k      0  0:02:56  0:00:05  0:02:51 3144k\n",
      "  4  410M    4 18.5M    0     0  2738k      0  0:02:33  0:00:06  0:02:27 3872k\n",
      "  5  410M    5 22.7M    0     0  2932k      0  0:02:23  0:00:07  0:02:16 4291k\n",
      "  6  410M    6 27.3M    0     0  3130k      0  0:02:14  0:00:08  0:02:06 4700k\n",
      "  7  410M    7 31.6M    0     0  3261k      0  0:02:08  0:00:09  0:01:59 4614k\n",
      "  8  410M    8 36.0M    0     0  3370k      0  0:02:04  0:00:10  0:01:54 4547k\n",
      "  9  410M    9 40.2M    0     0  3446k      0  0:02:01  0:00:11  0:01:50 4431k\n",
      " 10  410M   10 44.7M    0     0  3536k      0  0:01:58  0:00:12  0:01:46 4496k\n",
      " 11  410M   11 46.9M    0     0  3444k      0  0:02:02  0:00:13  0:01:49 4005k\n",
      " 12  410M   12 52.7M    0     0  3611k      0  0:01:56  0:00:14  0:01:42 4307k\n",
      " 13  410M   13 56.6M    0     0  3635k      0  0:01:55  0:00:15  0:01:40 4213k\n",
      " 14  410M   14 61.0M    0     0  3689k      0  0:01:53  0:00:16  0:01:37 4267k\n",
      " 15  410M   15 65.1M    0     0  3717k      0  0:01:53  0:00:17  0:01:36 4187k\n",
      " 16  410M   16 69.0M    0     0  3730k      0  0:01:52  0:00:18  0:01:34 4529k\n",
      " 17  410M   17 73.2M    0     0  3756k      0  0:01:51  0:00:19  0:01:32 4190k\n",
      " 18  410M   18 77.4M    0     0  3788k      0  0:01:50  0:00:20  0:01:30 4277k\n",
      " 19  410M   19 81.7M    0     0  3813k      0  0:01:50  0:00:21  0:01:29 4233k\n",
      " 21  410M   21 86.6M    0     0  3865k      0  0:01:48  0:00:22  0:01:26 4395k\n",
      " 22  410M   22 90.4M    0     0  3868k      0  0:01:48  0:00:23  0:01:25 4393k\n",
      " 23  410M   23 94.6M    0     0  3883k      0  0:01:48  0:00:24  0:01:24 4391k\n",
      " 24  410M   24 98.9M    0     0  3905k      0  0:01:47  0:00:25  0:01:22 4396k\n",
      " 24  410M   24  102M    0     0  3887k      0  0:01:48  0:00:26  0:01:22 4213k\n",
      " 26  410M   26  107M    0     0  3923k      0  0:01:47  0:00:27  0:01:20 4192k\n",
      " 27  410M   27  111M    0     0  3941k      0  0:01:46  0:00:28  0:01:18 4291k\n",
      " 28  410M   28  116M    0     0  3973k      0  0:01:45  0:00:29  0:01:16 4422k\n",
      " 29  410M   29  120M    0     0  3987k      0  0:01:45  0:00:30  0:01:15 4414k\n",
      " 30  410M   30  123M    0     0  3930k      0  0:01:46  0:00:32  0:01:14 4155k\n",
      " 31  410M   31  128M    0     0  3996k      0  0:01:45  0:00:32  0:01:13 4406k\n",
      " 32  410M   32  132M    0     0  4004k      0  0:01:45  0:00:33  0:01:12 4368k\n",
      " 33  410M   33  137M    0     0  4015k      0  0:01:44  0:00:34  0:01:10 4268k\n",
      " 34  410M   34  141M    0     0  4025k      0  0:01:44  0:00:35  0:01:09 4259k\n",
      " 35  410M   35  145M    0     0  4035k      0  0:01:44  0:00:36  0:01:08 4743k\n",
      " 36  410M   36  149M    0     0  4047k      0  0:01:43  0:00:37  0:01:06 4380k\n",
      " 37  410M   37  153M    0     0  4044k      0  0:01:43  0:00:38  0:01:05 4316k\n",
      " 37  410M   37  154M    0     0  3941k      0  0:01:46  0:00:40  0:01:06 3443k\n",
      " 37  410M   37  154M    0     0  3845k      0  0:01:49  0:00:41  0:01:08 2603k\n",
      " 37  410M   37  154M    0     0  3752k      0  0:01:52  0:00:42  0:01:10 1753k\n",
      " 37  410M   37  154M    0     0  3665k      0  0:01:54  0:00:43  0:01:11  900k\n",
      " 37  410M   37  154M    0     0  3581k      0  0:01:57  0:00:44  0:01:13  149k\n",
      " 37  410M   37  154M    0     0  3500k      0  0:02:00  0:00:45  0:01:15     0\n",
      " 37  410M   37  154M    0     0  3424k      0  0:02:02  0:00:46  0:01:16     0\n",
      " 37  410M   37  154M    0     0  3351k      0  0:02:05  0:00:47  0:01:18     0\n",
      " 37  410M   37  154M    0     0  3281k      0  0:02:08  0:00:48  0:01:20     0\n",
      " 37  410M   37  154M    0     0  3214k      0  0:02:10  0:00:49  0:01:21     0\n",
      " 37  410M   37  154M    0     0  3149k      0  0:02:13  0:00:50  0:01:23     0\n",
      " 37  410M   37  154M    0     0  3087k      0  0:02:16  0:00:51  0:01:25     0\n",
      " 37  410M   37  154M    0     0  3027k      0  0:02:18  0:00:52  0:01:26     0\n",
      " 37  410M   37  154M    0     0  2970k      0  0:02:21  0:00:53  0:01:28     0\n",
      " 37  410M   37  154M    0     0  2915k      0  0:02:24  0:00:54  0:01:30     0\n",
      " 37  410M   37  154M    0     0  2862k      0  0:02:26  0:00:55  0:01:31     0\n",
      " 37  410M   37  154M    0     0  2810k      0  0:02:29  0:00:56  0:01:33     0\n",
      " 37  410M   37  154M    0     0  2760k      0  0:02:32  0:00:57  0:01:35     0\n",
      " 37  410M   37  154M    0     0  2713k      0  0:02:34  0:00:58  0:01:36     0\n",
      " 37  410M   37  154M    0     0  2667k      0  0:02:37  0:00:59  0:01:38     0\n",
      " 37  410M   37  154M    0     0  2622k      0  0:02:40  0:01:00  0:01:40     0\n",
      " 37  410M   37  154M    0     0  2579k      0  0:02:43  0:01:01  0:01:42     0\n",
      " 37  410M   37  154M    0     0  2537k      0  0:02:45  0:01:02  0:01:43     0\n",
      " 37  410M   37  154M    0     0  2497k      0  0:02:48  0:01:03  0:01:45     0\n",
      " 37  410M   37  154M    0     0  2457k      0  0:02:51  0:01:04  0:01:47     0\n",
      " 37  410M   37  154M    0     0  2419k      0  0:02:53  0:01:05  0:01:48     0\n",
      " 37  410M   37  154M    0     0  2383k      0  0:02:56  0:01:06  0:01:50     0\n",
      " 37  410M   37  154M    0     0  2347k      0  0:02:59  0:01:07  0:01:52     0\n",
      " 37  410M   37  154M    0     0  2312k      0  0:03:01  0:01:08  0:01:53     0\n",
      " 37  410M   37  154M    0     0  2279k      0  0:03:04  0:01:09  0:01:55     0\n",
      " 37  410M   37  154M    0     0  2246k      0  0:03:07  0:01:10  0:01:57     0\n",
      " 37  410M   37  154M    0     0  2214k      0  0:03:09  0:01:11  0:01:58     0\n",
      " 37  410M   37  154M    0     0  2183k      0  0:03:12  0:01:12  0:02:00     0\n",
      " 37  410M   37  154M    0     0  2154k      0  0:03:15  0:01:13  0:02:02     0\n",
      " 37  410M   37  154M    0     0  2125k      0  0:03:17  0:01:14  0:02:03     0\n",
      " 37  410M   37  154M    0     0  2096k      0  0:03:20  0:01:15  0:02:05     0\n",
      " 37  410M   37  154M    0     0  2068k      0  0:03:23  0:01:16  0:02:07     0\n",
      " 37  410M   37  154M    0     0  2041k      0  0:03:25  0:01:17  0:02:08     0\n",
      " 37  410M   37  154M    0     0  2015k      0  0:03:28  0:01:18  0:02:10     0\n",
      " 37  410M   37  154M    0     0  1990k      0  0:03:31  0:01:19  0:02:12     0\n",
      " 37  410M   37  154M    0     0  1964k      0  0:03:34  0:01:20  0:02:14     0\n",
      " 37  410M   37  154M    0     0  1940k      0  0:03:36  0:01:21  0:02:15     0\n",
      " 37  410M   37  154M    0     0  1916k      0  0:03:39  0:01:22  0:02:17     0\n",
      " 37  410M   37  154M    0     0  1893k      0  0:03:42  0:01:23  0:02:19     0\n",
      " 37  410M   37  154M    0     0  1870k      0  0:03:44  0:01:24  0:02:20     0\n",
      " 37  410M   37  154M    0     0  1848k      0  0:03:47  0:01:25  0:02:22     0\n",
      " 37  410M   37  154M    0     0  1827k      0  0:03:50  0:01:26  0:02:24     0\n",
      " 37  410M   37  154M    0     0  1806k      0  0:03:52  0:01:27  0:02:25     0\n",
      " 37  410M   37  154M    0     0  1786k      0  0:03:55  0:01:28  0:02:27     0\n",
      " 37  410M   37  154M    0     0  1766k      0  0:03:58  0:01:29  0:02:29     0\n",
      " 37  410M   37  154M    0     0  1746k      0  0:04:00  0:01:30  0:02:30     0\n",
      " 37  410M   37  154M    0     0  1727k      0  0:04:03  0:01:31  0:02:32     0\n",
      " 37  410M   37  154M    0     0  1708k      0  0:04:06  0:01:32  0:02:34     0\n",
      " 37  410M   37  154M    0     0  1690k      0  0:04:08  0:01:33  0:02:35     0\n",
      " 37  410M   37  154M    0     0  1672k      0  0:04:11  0:01:34  0:02:37     0\n",
      " 37  410M   37  154M    0     0  1654k      0  0:04:14  0:01:35  0:02:39     0\n",
      " 37  410M   37  154M    0     0  1637k      0  0:04:16  0:01:36  0:02:40     0\n",
      " 37  410M   37  154M    0     0  1620k      0  0:04:19  0:01:37  0:02:42     0\n",
      " 37  410M   37  154M    0     0  1603k      0  0:04:22  0:01:38  0:02:44     0\n",
      " 37  410M   37  154M    0     0  1588k      0  0:04:24  0:01:39  0:02:45  3271\n",
      " 37  410M   37  154M    0     0  1583k      0  0:04:25  0:01:40  0:02:45 11361\n",
      " 37  410M   37  155M    0     0  1572k      0  0:04:27  0:01:40  0:02:47  116k\n",
      " 37  410M   37  155M    0     0  1565k      0  0:04:28  0:01:41  0:02:47  297k\n",
      " 38  410M   38  157M    0     0  1569k      0  0:04:27  0:01:42  0:02:45  779k\n",
      " 38  410M   38  159M    0     0  1567k      0  0:04:28  0:01:43  0:02:45 1075k\n",
      " 39  410M   39  162M    0     0  1583k      0  0:04:25  0:01:44  0:02:41 1584k\n",
      " 39  410M   39  164M    0     0  1585k      0  0:04:25  0:01:45  0:02:40 1838k\n",
      " 40  410M   40  165M    0     0  1592k      0  0:04:23  0:01:46  0:02:37 2213k\n",
      "curl: (18) transfer closed with 256928258 bytes remaining to read\n"
     ]
    }
   ],
   "source": [
    "!curl \"https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-3687/ud-treebanks-v2.8.tgz?sequence=1&isAllowed=y\" --output my_ud_file.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncompress the archive and:\n",
    "1. Go to the Swedish _Talbanken_ corpus;\n",
    "2. Read the CoNLL-U annotation here: https://universaldependencies.org/format.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Examining the annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will carry out the following steps and describe them in your report:\n",
    "\n",
    "1. Draw graphical representations of the two first Swedish sentences of the training set. You will include these drawings in your report;\n",
    "2. Visualize these sentences with this tool: http://spyysalo.github.io/conllu.js/ and check that you have the same results;\n",
    "3. Apply the dependency parser for Swedish of the <a href=\"http://vilde.cs.lth.se:9000/\">Langforia pipelines</a> to these sentences (only the text of each sentence). You will have to select Swedish and activate both `Token` and `DependencyRelation`. Link to Lanforia pipelines: <a href=\"http://vilde.cs.lth.se:9000/\">http://vilde.cs.lth.se:9000/</a>. You will describe possible differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Swedish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will extract all the subject–verb pairs and the subject–verb–object triples from the Swedish _Talbanken_ training corpus. To start the program, you can use the CoNLL-U reader available in the cells below.\n",
    "This program works for the other corpora. You can also program a reader yourself starting from the one you used to read the CoNLL 2000 format in the fourth lab or from scratch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the corpus locations you will use. You may have to adjust `ud_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ud_path = 'ud-treebanks-v2.8/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_sv = ud_path + 'UD_Swedish-Talbanken/sv_talbanken-ud-train.conllu'\n",
    "path_fr = ud_path + 'UD_French-GSD/fr_gsd-ud-train.conllu'\n",
    "path_ru = ud_path + 'UD_Russian-SynTagRus/ru_syntagrus-ud-train.conllu'\n",
    "path_en = ud_path + 'UD_English-EWT/en_ewt-ud-train.conllu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column names of the CoNLL-U corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names_u = ['ID', 'FORM', 'LEMMA', 'UPOS', 'XPOS', 'FEATS', 'HEAD', 'DEPREL', 'DEPS', 'MISC']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to read the CoNLL-U files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentences(file):\n",
    "    \"\"\"\n",
    "    Creates a list of sentences from the corpus\n",
    "    Each sentence is a string\n",
    "    :param file:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    f = open(file, encoding='utf-8').read().strip()\n",
    "    sentences = f.split('\\n\\n')\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_rows(sentences, column_names):\n",
    "    \"\"\"\n",
    "    Creates a list of sentence where each sentence is a list of lines\n",
    "    Each line is a dictionary of columns\n",
    "    :param sentences:\n",
    "    :param column_names:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    new_sentences = []\n",
    "    root_values = ['0', 'ROOT', 'ROOT', 'ROOT', 'ROOT', 'ROOT', '0', 'ROOT', '0', 'ROOT']\n",
    "    start = [dict(zip(column_names, root_values))]\n",
    "    for sentence in sentences:\n",
    "        rows = sentence.split('\\n')\n",
    "        sentence = [dict(zip(column_names, row.split('\\t'))) for row in rows if row[0] != '#']\n",
    "        sentence = start + sentence\n",
    "        new_sentences.append(sentence)\n",
    "    return new_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Reading the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the Swedish _Talbanken_ corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = read_sentences(path_sv)\n",
    "formatted_corpus = split_rows(sentences, column_names_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = read_sentences(path_sv)\n",
    "formatted_corpus = split_rows(sentences, column_names_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4303"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(formatted_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parsed sentence: _Genom skattereformen införs individuell beskattning (särbeskattning) av arbetsinkomster._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ID': '0',\n",
       "  'FORM': 'ROOT',\n",
       "  'LEMMA': 'ROOT',\n",
       "  'UPOS': 'ROOT',\n",
       "  'XPOS': 'ROOT',\n",
       "  'FEATS': 'ROOT',\n",
       "  'HEAD': '0',\n",
       "  'DEPREL': 'ROOT',\n",
       "  'DEPS': '0',\n",
       "  'MISC': 'ROOT'},\n",
       " {'ID': '1',\n",
       "  'FORM': 'Genom',\n",
       "  'LEMMA': 'genom',\n",
       "  'UPOS': 'ADP',\n",
       "  'XPOS': 'PP',\n",
       "  'FEATS': '_',\n",
       "  'HEAD': '2',\n",
       "  'DEPREL': 'case',\n",
       "  'DEPS': '2:case',\n",
       "  'MISC': '_'},\n",
       " {'ID': '2',\n",
       "  'FORM': 'skattereformen',\n",
       "  'LEMMA': 'skattereform',\n",
       "  'UPOS': 'NOUN',\n",
       "  'XPOS': 'NN|UTR|SIN|DEF|NOM',\n",
       "  'FEATS': 'Case=Nom|Definite=Def|Gender=Com|Number=Sing',\n",
       "  'HEAD': '3',\n",
       "  'DEPREL': 'obl',\n",
       "  'DEPS': '3:obl:genom',\n",
       "  'MISC': '_'},\n",
       " {'ID': '3',\n",
       "  'FORM': 'införs',\n",
       "  'LEMMA': 'införa',\n",
       "  'UPOS': 'VERB',\n",
       "  'XPOS': 'VB|PRS|SFO',\n",
       "  'FEATS': 'Mood=Ind|Tense=Pres|VerbForm=Fin|Voice=Pass',\n",
       "  'HEAD': '0',\n",
       "  'DEPREL': 'root',\n",
       "  'DEPS': '0:root',\n",
       "  'MISC': '_'},\n",
       " {'ID': '4',\n",
       "  'FORM': 'individuell',\n",
       "  'LEMMA': 'individuell',\n",
       "  'UPOS': 'ADJ',\n",
       "  'XPOS': 'JJ|POS|UTR|SIN|IND|NOM',\n",
       "  'FEATS': 'Case=Nom|Definite=Ind|Degree=Pos|Gender=Com|Number=Sing',\n",
       "  'HEAD': '5',\n",
       "  'DEPREL': 'amod',\n",
       "  'DEPS': '5:amod',\n",
       "  'MISC': '_'},\n",
       " {'ID': '5',\n",
       "  'FORM': 'beskattning',\n",
       "  'LEMMA': 'beskattning',\n",
       "  'UPOS': 'NOUN',\n",
       "  'XPOS': 'NN|UTR|SIN|IND|NOM',\n",
       "  'FEATS': 'Case=Nom|Definite=Ind|Gender=Com|Number=Sing',\n",
       "  'HEAD': '3',\n",
       "  'DEPREL': 'nsubj:pass',\n",
       "  'DEPS': '3:nsubj:pass',\n",
       "  'MISC': '_'},\n",
       " {'ID': '6',\n",
       "  'FORM': '(',\n",
       "  'LEMMA': '(',\n",
       "  'UPOS': 'PUNCT',\n",
       "  'XPOS': 'PAD',\n",
       "  'FEATS': '_',\n",
       "  'HEAD': '5',\n",
       "  'DEPREL': 'punct',\n",
       "  'DEPS': '5:punct',\n",
       "  'MISC': 'SpaceAfter=No'},\n",
       " {'ID': '7',\n",
       "  'FORM': 'särbeskattning',\n",
       "  'LEMMA': 'särbeskattning',\n",
       "  'UPOS': 'NOUN',\n",
       "  'XPOS': 'NN|UTR|SIN|IND|NOM',\n",
       "  'FEATS': 'Case=Nom|Definite=Ind|Gender=Com|Number=Sing',\n",
       "  'HEAD': '5',\n",
       "  'DEPREL': 'appos',\n",
       "  'DEPS': '5:appos',\n",
       "  'MISC': 'SpaceAfter=No'},\n",
       " {'ID': '8',\n",
       "  'FORM': ')',\n",
       "  'LEMMA': ')',\n",
       "  'UPOS': 'PUNCT',\n",
       "  'XPOS': 'PAD',\n",
       "  'FEATS': '_',\n",
       "  'HEAD': '5',\n",
       "  'DEPREL': 'punct',\n",
       "  'DEPS': '5:punct',\n",
       "  'MISC': '_'},\n",
       " {'ID': '9',\n",
       "  'FORM': 'av',\n",
       "  'LEMMA': 'av',\n",
       "  'UPOS': 'ADP',\n",
       "  'XPOS': 'PP',\n",
       "  'FEATS': '_',\n",
       "  'HEAD': '10',\n",
       "  'DEPREL': 'case',\n",
       "  'DEPS': '10:case',\n",
       "  'MISC': '_'},\n",
       " {'ID': '10',\n",
       "  'FORM': 'arbetsinkomster',\n",
       "  'LEMMA': 'arbetsinkomst',\n",
       "  'UPOS': 'NOUN',\n",
       "  'XPOS': 'NN|UTR|PLU|IND|NOM',\n",
       "  'FEATS': 'Case=Nom|Definite=Ind|Gender=Com|Number=Plur',\n",
       "  'HEAD': '5',\n",
       "  'DEPREL': 'nmod',\n",
       "  'DEPS': '5:nmod:av',\n",
       "  'MISC': 'SpaceAfter=No'},\n",
       " {'ID': '11',\n",
       "  'FORM': '.',\n",
       "  'LEMMA': '.',\n",
       "  'UPOS': 'PUNCT',\n",
       "  'XPOS': 'MAD',\n",
       "  'FEATS': '_',\n",
       "  'HEAD': '3',\n",
       "  'DEPREL': 'punct',\n",
       "  'DEPS': '3:punct',\n",
       "  'MISC': '_'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_corpus[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting the lists in dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ease the processing of some corpora, you will use a dictionary represention of the sentences. The keys will be the `ID` values. We do this because `ID` is not necessarily a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_dict(formatted_corpus):\n",
    "    \"\"\"\n",
    "    Converts each sentence from a list of words to a dictionary where the keys are id\n",
    "    :param formatted_corpus:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    formatted_corpus_dict = []\n",
    "    for sentence in formatted_corpus:\n",
    "        sentence_dict = {}\n",
    "        for word in sentence:\n",
    "            sentence_dict[word['ID']] = word\n",
    "        formatted_corpus_dict.append(sentence_dict)\n",
    "    return formatted_corpus_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': {'ID': '0',\n",
       "  'FORM': 'ROOT',\n",
       "  'LEMMA': 'ROOT',\n",
       "  'UPOS': 'ROOT',\n",
       "  'XPOS': 'ROOT',\n",
       "  'FEATS': 'ROOT',\n",
       "  'HEAD': '0',\n",
       "  'DEPREL': 'ROOT',\n",
       "  'DEPS': '0',\n",
       "  'MISC': 'ROOT'},\n",
       " '1': {'ID': '1',\n",
       "  'FORM': 'Genom',\n",
       "  'LEMMA': 'genom',\n",
       "  'UPOS': 'ADP',\n",
       "  'XPOS': 'PP',\n",
       "  'FEATS': '_',\n",
       "  'HEAD': '2',\n",
       "  'DEPREL': 'case',\n",
       "  'DEPS': '2:case',\n",
       "  'MISC': '_'},\n",
       " '2': {'ID': '2',\n",
       "  'FORM': 'skattereformen',\n",
       "  'LEMMA': 'skattereform',\n",
       "  'UPOS': 'NOUN',\n",
       "  'XPOS': 'NN|UTR|SIN|DEF|NOM',\n",
       "  'FEATS': 'Case=Nom|Definite=Def|Gender=Com|Number=Sing',\n",
       "  'HEAD': '3',\n",
       "  'DEPREL': 'obl',\n",
       "  'DEPS': '3:obl:genom',\n",
       "  'MISC': '_'},\n",
       " '3': {'ID': '3',\n",
       "  'FORM': 'införs',\n",
       "  'LEMMA': 'införa',\n",
       "  'UPOS': 'VERB',\n",
       "  'XPOS': 'VB|PRS|SFO',\n",
       "  'FEATS': 'Mood=Ind|Tense=Pres|VerbForm=Fin|Voice=Pass',\n",
       "  'HEAD': '0',\n",
       "  'DEPREL': 'root',\n",
       "  'DEPS': '0:root',\n",
       "  'MISC': '_'},\n",
       " '4': {'ID': '4',\n",
       "  'FORM': 'individuell',\n",
       "  'LEMMA': 'individuell',\n",
       "  'UPOS': 'ADJ',\n",
       "  'XPOS': 'JJ|POS|UTR|SIN|IND|NOM',\n",
       "  'FEATS': 'Case=Nom|Definite=Ind|Degree=Pos|Gender=Com|Number=Sing',\n",
       "  'HEAD': '5',\n",
       "  'DEPREL': 'amod',\n",
       "  'DEPS': '5:amod',\n",
       "  'MISC': '_'},\n",
       " '5': {'ID': '5',\n",
       "  'FORM': 'beskattning',\n",
       "  'LEMMA': 'beskattning',\n",
       "  'UPOS': 'NOUN',\n",
       "  'XPOS': 'NN|UTR|SIN|IND|NOM',\n",
       "  'FEATS': 'Case=Nom|Definite=Ind|Gender=Com|Number=Sing',\n",
       "  'HEAD': '3',\n",
       "  'DEPREL': 'nsubj:pass',\n",
       "  'DEPS': '3:nsubj:pass',\n",
       "  'MISC': '_'},\n",
       " '6': {'ID': '6',\n",
       "  'FORM': '(',\n",
       "  'LEMMA': '(',\n",
       "  'UPOS': 'PUNCT',\n",
       "  'XPOS': 'PAD',\n",
       "  'FEATS': '_',\n",
       "  'HEAD': '5',\n",
       "  'DEPREL': 'punct',\n",
       "  'DEPS': '5:punct',\n",
       "  'MISC': 'SpaceAfter=No'},\n",
       " '7': {'ID': '7',\n",
       "  'FORM': 'särbeskattning',\n",
       "  'LEMMA': 'särbeskattning',\n",
       "  'UPOS': 'NOUN',\n",
       "  'XPOS': 'NN|UTR|SIN|IND|NOM',\n",
       "  'FEATS': 'Case=Nom|Definite=Ind|Gender=Com|Number=Sing',\n",
       "  'HEAD': '5',\n",
       "  'DEPREL': 'appos',\n",
       "  'DEPS': '5:appos',\n",
       "  'MISC': 'SpaceAfter=No'},\n",
       " '8': {'ID': '8',\n",
       "  'FORM': ')',\n",
       "  'LEMMA': ')',\n",
       "  'UPOS': 'PUNCT',\n",
       "  'XPOS': 'PAD',\n",
       "  'FEATS': '_',\n",
       "  'HEAD': '5',\n",
       "  'DEPREL': 'punct',\n",
       "  'DEPS': '5:punct',\n",
       "  'MISC': '_'},\n",
       " '9': {'ID': '9',\n",
       "  'FORM': 'av',\n",
       "  'LEMMA': 'av',\n",
       "  'UPOS': 'ADP',\n",
       "  'XPOS': 'PP',\n",
       "  'FEATS': '_',\n",
       "  'HEAD': '10',\n",
       "  'DEPREL': 'case',\n",
       "  'DEPS': '10:case',\n",
       "  'MISC': '_'},\n",
       " '10': {'ID': '10',\n",
       "  'FORM': 'arbetsinkomster',\n",
       "  'LEMMA': 'arbetsinkomst',\n",
       "  'UPOS': 'NOUN',\n",
       "  'XPOS': 'NN|UTR|PLU|IND|NOM',\n",
       "  'FEATS': 'Case=Nom|Definite=Ind|Gender=Com|Number=Plur',\n",
       "  'HEAD': '5',\n",
       "  'DEPREL': 'nmod',\n",
       "  'DEPS': '5:nmod:av',\n",
       "  'MISC': 'SpaceAfter=No'},\n",
       " '11': {'ID': '11',\n",
       "  'FORM': '.',\n",
       "  'LEMMA': '.',\n",
       "  'UPOS': 'PUNCT',\n",
       "  'XPOS': 'MAD',\n",
       "  'FEATS': '_',\n",
       "  'HEAD': '3',\n",
       "  'DEPREL': 'punct',\n",
       "  'DEPS': '3:punct',\n",
       "  'MISC': '_'}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_corpus_dict = convert_to_dict(formatted_corpus)\n",
    "formatted_corpus_dict[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the subject-verb pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will extract the subject-verb pairs, where you will set the words in lowercase. In the second sentence of the corpus, this corresponds to `(beskattning, införs)`. You will call the function `extract_pairs(formatted_corpus_dict)` and and you will store the results in a `pairs_sv` variable. All the corpora in the universal dependencies format use the same names for the grammatical functions. The subject and direct object are respectively `nsubj` and `obj`.\n",
    "\n",
    "You can use the algorithm you want. However, here are some hints on the results:\n",
    "* You will extract all the subject-verb pairs in the corpus. In the extraction, just check the function between two words. Do not check if the part of speech is a verb or a noun in the pair. You will also ignore the possible function suffixes as in `nsubj:pass`, where `pass` means passive.\n",
    "* You will return the results as Python's dictionaries, where the key will be the pair and the value, the count, as for instance `{(beskattning, införs): 1}`. Be sure you understand the Python dictionaries and note that you can use tuples as keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pairs(formatted_corpus_dict):\n",
    "    pairs_sv = {}\n",
    "    for i,sentence in enumerate(formatted_corpus_dict):\n",
    "        for word in sentence:\n",
    "            if formatted_corpus_dict[i][word]['DEPREL'].startswith('nsubj'):\n",
    "                verb = formatted_corpus_dict[i][word]['HEAD'] #verb connected with nsubj\n",
    "                pair = (formatted_corpus_dict[i][word]['FORM'].lower(),formatted_corpus_dict[i][verb]['FORM'].lower())\n",
    "                if pair in pairs_sv:\n",
    "                    pairs_sv[pair] += 1\n",
    "                else:\n",
    "                    pairs_sv[pair] = 1\n",
    "    return pairs_sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_sv = extract_pairs(formatted_corpus_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will compute the total number of subject-verb pairs. You should find 6,083 pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6083"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([pairs_sv[pair] for pair in pairs_sv])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the most frequent pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will sort your pairs by frequency and by lexical order of the pairs and store the five most frequent pairs in the `freq_pairs_sv` variable as in:\n",
    "```\n",
    "freq_pairs_sv = [(('som', 'har'), 45),\n",
    "\n",
    " ...]\n",
    " ````\n",
    "\n",
    "Here are the frequencies you should find:\n",
    "```\n",
    "45\n",
    "19\n",
    "19\n",
    "```\n",
    "When pairs have equal frequencies, you will sort them by alphabetical order, first term, and then second term of the pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all the experiments, we will keep the `nbest` most frequent. In the first experiments, we set `nbest` to 3 first. We will set it to 5 in the last experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbest = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_pairs = sorted(pairs_sv, key=lambda x: (-pairs_sv[x], x))\n",
    "freq_pairs_sv = [(pair, pairs_sv[pair]) for pair in sorted_pairs][:nbest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('som', 'har'), 45), (('du', 'får'), 19), (('vi', 'har'), 19)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_pairs_sv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the subject-verb-object triples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now extract all the subject–verb–object triples of the corpus. The object function uses the `obj` code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_triples(formatted_corpus_dict):\n",
    "    triples_sv = {}\n",
    "    for i,sentence in enumerate(formatted_corpus_dict):\n",
    "        for word in sentence:\n",
    "            if formatted_corpus_dict[i][word]['DEPREL'].startswith('nsubj'):\n",
    "                verb = formatted_corpus_dict[i][word]['HEAD'] #verb connected with nsubj\n",
    "                first = formatted_corpus_dict[i][word]['FORM'].lower()\n",
    "                second = formatted_corpus_dict[i][verb]['FORM'].lower()\n",
    "                for word2 in sentence: #Iterate again and look for object\n",
    "                    obj = formatted_corpus_dict[i][word2]['DEPREL']\n",
    "                    if obj.startswith('obj'):\n",
    "                        third = formatted_corpus_dict[i][word2]['FORM'].lower()\n",
    "                        if verb == formatted_corpus_dict[i][word2]['HEAD']: # pointing at the same verb?\n",
    "                            pair = (first, second, third)\n",
    "                            if pair in triples_sv:\n",
    "                                triples_sv[pair] += 1\n",
    "                            else:\n",
    "                                triples_sv[pair] = 1\n",
    "    return triples_sv   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "triples_sv = extract_triples(formatted_corpus_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the total number of triples. You should find 2054 triples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2054"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([triples_sv[triple] for triple in triples_sv])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the most frequent triples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will sort your triples by frequency and by lexical order of the pairs and store the three most frequent triples in the `freq_triples_sv` variable as in:\n",
    "```\n",
    "freq_triples_sv = [(('man', 'vänder', 'sig'), 14),\n",
    "\n",
    " ...]\n",
    " ````\n",
    "\n",
    "Here are the frequencies you should find:\n",
    "```\n",
    "14\n",
    "5\n",
    "3\n",
    "```\n",
    "As with the pairs of equal frequencies, you will sort the triples by alphabetical order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_triples = sorted(triples_sv, key=lambda x: (-triples_sv[x], x))\n",
    "freq_triples_sv = [(triple, triples_sv[triple]) for triple in sorted_triples][:nbest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('man', 'vänder', 'sig'), 14),\n",
       " (('det', 'rör', 'sig'), 5),\n",
       " (('man', 'söker', 'arbete'), 3)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_triples_sv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilingual Corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your program is working on Swedish, you will apply it to all the other languages in universal dependencies. The code below returns all the files from a folder with a suffix. Here we consider the training files only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(dir, suffix):\n",
    "    \"\"\"\n",
    "    Returns all the files in a folder ending with suffix\n",
    "    Recursive version\n",
    "    :param dir:\n",
    "    :param suffix:\n",
    "    :return: the list of file names\n",
    "    \"\"\"\n",
    "    files = []\n",
    "    for file in os.listdir(dir):\n",
    "        path = dir + '/' + file\n",
    "        if os.path.isdir(path):\n",
    "            files += get_files(path, suffix)\n",
    "        elif os.path.isfile(path) and file.endswith(suffix):\n",
    "            files.append(path)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = get_files(ud_path, 'train.conllu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with the indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some corpora expand some tokens into multiwords. This is the case in French, Spanish, German, as well as English in some corpora.\n",
    "        The table below shows examples of such expansions.\n",
    "        <table style=\"width:100%\">\n",
    "            <tr>\n",
    "                <th>French</th>\n",
    "                <th>Spanish</th>\n",
    "                <th>German</th>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td><i>du</i>: de le\n",
    "                </td>\n",
    "                <td><i>del</i>: de el\n",
    "                </td>\n",
    "                <td><i>zur</i>: zu der\n",
    "                </td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td><i>des</i>: de les\n",
    "                </td>\n",
    "                <td><i>vámonos</i>: vamos nos\n",
    "                </td>\n",
    "                <td><i>im</i>: in dem\n",
    "                </td>\n",
    "            </tr>\n",
    "        </table>\n",
    "        In the corpora, you have the original tokens as well as the multiwords as with <i>vámonos al mar</i> as in:\n",
    "        <pre>\n",
    "1-2 vámonos _\n",
    "1 vamos ir\n",
    "2 nos nosotros\n",
    "3-4 al _\n",
    "3 a a\n",
    "4 el el\n",
    "5 mar mar\n",
    "</pre>Read the format description for the details: [<a\n",
    "                href=\"http://universaldependencies.org/format.html\">CoNLL-U format</a>]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you represent the sentences as lists, the item indices are not reliable: In the format description,\n",
    "        the token at position 1 is <i>vamos</i> and not <i>vámonos</i>.\n",
    "        You have two ways to cope with this:\n",
    "1. Either remove all the lines that include a range in the `ID` field, or\n",
    "2. Encode the sentences as dictionaries (I felt this was preferable), where the keys are the `ID` numbers. This is what `convert_to_dict()` does. Here are the results for a sentence from the French CoNLL-U corpus:\n",
    "_Les iris du mâles sont jaunes toute l'année._ Note the `3-4` index and it expansion in `3`and `4`:\n",
    "```\n",
    "{'0': {'ID': '0',  'FORM': 'ROOT',  'LEMMA': 'ROOT',  'UPOS': 'ROOT',  'XPOS': 'ROOT',  'FEATS': 'ROOT',  'HEAD': '0',  'DEPREL': 'ROOT',  'DEPS': '0',  'MISC': 'ROOT'}, \n",
    "'1': {'ID': '1',  'FORM': 'Les',  'LEMMA': 'le',  'UPOS': 'DET',  'XPOS': '_',  'FEATS': 'Definite=Def|Gender=Masc|Number=Plur|PronType=Art',  'HEAD': '2',  'DEPREL': 'det',  'DEPS': '_',  'MISC': 'wordform=les'}, \n",
    "'2': {'ID': '2',  'FORM': 'iris',  'LEMMA': 'iris',  'UPOS': 'NOUN',  'XPOS': '_',  'FEATS': 'Gender=Masc|Number=Plur',  'HEAD': '7',  'DEPREL': 'nsubj',  'DEPS': '_',  'MISC': '_'}, \n",
    "'3-4': {'ID': '3-4',  'FORM': 'du',  'LEMMA': '_',  'UPOS': '_',  'XPOS': '_',  'FEATS': '_',  'HEAD': '_',  'DEPREL': '_',  'DEPS': '_',  'MISC': '_'}, \n",
    "'3': {'ID': '3',  'FORM': 'de',  'LEMMA': 'de',  'UPOS': 'ADP',  'XPOS': '_',  'FEATS': '_',  'HEAD': '5',  'DEPREL': 'case',  'DEPS': '_',  'MISC': '_'}, \n",
    "'4': {'ID': '4',  'FORM': 'le',  'LEMMA': 'le',  'UPOS': 'DET',  'XPOS': '_',  'FEATS': 'Definite=Def|Gender=Masc|Number=Sing|PronType=Art',  'HEAD': '5',  'DEPREL': 'det',  'DEPS': '_',  'MISC': '_'}, \n",
    "'5': {'ID': '5',  'FORM': 'mâles',  'LEMMA': 'mâle',  'UPOS': 'NOUN',  'XPOS': '_',  'FEATS': 'Gender=Masc|Number=Plur',  'HEAD': '2',  'DEPREL': 'nmod',  'DEPS': '_',  'MISC': '_'}, \n",
    "'6': {'ID': '6',  'FORM': 'sont',  'LEMMA': 'être',  'UPOS': 'AUX',  'XPOS': '_',  'FEATS': 'Mood=Ind|Number=Plur|Person=3|Tense=Pres|VerbForm=Fin',  'HEAD': '7',  'DEPREL': 'cop',  'DEPS': '_',  'MISC': '_'}, \n",
    "'7': {'ID': '7',  'FORM': 'jaunes',  'LEMMA': 'jaune',  'UPOS': 'ADJ',  'XPOS': '_',  'FEATS': 'Gender=Masc|Number=Plur',  'HEAD': '0',  'DEPREL': 'root',  'DEPS': '_',  'MISC': '_'}, \n",
    "'8': {'ID': '8',  'FORM': 'toute',  'LEMMA': 'tout',  'UPOS': 'ADJ',  'XPOS': '_',  'FEATS': 'Gender=Fem|Number=Sing',  'HEAD': '10',  'DEPREL': 'amod',  'DEPS': '_',  'MISC': '_'}, \n",
    "'9': {'ID': '9',  'FORM': \"l'\",  'LEMMA': 'le',  'UPOS': 'DET',  'XPOS': '_',  'FEATS': 'Definite=Def|Gender=Fem|Number=Sing|PronType=Art',  'HEAD': '10',  'DEPREL': 'det',  'DEPS': '_',  'MISC': 'SpaceAfter=No'}, \n",
    "'10': {'ID': '10',  'FORM': 'année',  'LEMMA': 'année',  'UPOS': 'NOUN',  'XPOS': '_',  'FEATS': 'Gender=Fem|Number=Sing',  'HEAD': '7',  'DEPREL': 'obl',  'DEPS': '_',  'MISC': 'SpaceAfter=No'}, \n",
    "'11': {'ID': '11',  'FORM': '.',  'LEMMA': '.',  'UPOS': 'PUNCT',  'XPOS': '_',  'FEATS': '_',  'HEAD': '7',  'DEPREL': 'punct',  'DEPS': '_',  'MISC': '_'}}\n",
    "```\n",
    "3. Some corpora have sentence numbers marked with a `#` symbol. You will ignore them and discard the lines starting with a `#`. This is already done in the CoNLL reader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting the pairs and triples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `extract_pairs_and_triples(formatted_corpus_dict, nbest)` that extracts the `nbest` most frequent pairs and triples of a given corpus and returns two sorted lists of tuples: `frequent_pairs` and `frequent_triples`. You will sort them by frequency and then by alphabetical order of the pair or triple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pairs_and_triples(formatted_corpus_dict, nbest):\n",
    "    pairs = extract_pairs(formatted_corpus_dict)\n",
    "    sorted_pairs = sorted(pairs, key=lambda x: (-pairs[x], x))\n",
    "    frequent_pairs = [(pair, pairs[pair]) for pair in sorted_pairs][:nbest]\n",
    "    \n",
    "    triples = extract_triples(formatted_corpus_dict)\n",
    "    sorted_triples = sorted(triples, key=lambda x: (-triples[x], x))\n",
    "    frequent_triples = [(triple, triples[triple]) for triple in sorted_triples][:nbest]\n",
    "    \n",
    "    return frequent_pairs, frequent_triples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run your extractor on all the corpora. Note that some corpora have replaced the words by underscores as for the FTB corpus in French. This is because the text is copyrighted. You need then to contact the provider and sign a license to obtain them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('individuo', 'ha', 'diritto'), 22), (('sigla', 'significa', 'cosa'), 14), (('proprietario', 'ha', 'diritto'), 11)]\n",
      "[(('det', 'drejer', 'sig'), 8), (('de', 'taler', 'tyrkisk'), 3), (('han', 'gjorde', 'det'), 3)]\n",
      "[(('jeg', 'føler', 'meg'), 7), (('jeg', 'har', 'lyst'), 7), (('jeg', 'har', 'spørsmål'), 6)]\n"
     ]
    }
   ],
   "source": [
    "path_it = ud_path + 'UD_Italian-ISDT/it_isdt-ud-train.conllu'\n",
    "path_dk = ud_path + 'UD_Danish-DDT/da_ddt-ud-train.conllu'\n",
    "path_no = ud_path + 'UD_Norwegian-Bokmaal/no_bokmaal-ud-train.conllu'\n",
    "paths = [path_it, path_dk, path_no]\n",
    "for language in paths:\n",
    "    sentences = read_sentences(language)\n",
    "    formatted_corpus = split_rows(sentences, column_names_u)\n",
    "    formatted_corpus_dict = convert_to_dict(formatted_corpus)\n",
    "    triples = extract_triples(formatted_corpus_dict)\n",
    "    sorted_triples = sorted(triples, key=lambda x: (-triples[x], x))\n",
    "    frequent_triples = [(triple, triples[triple]) for triple in sorted_triples][:nbest]\n",
    "    print(frequent_triples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your report, you will include the `nbest` most frequent pairs and triples you obtained in **three languages**. You may choose the ones you want.\n",
    "\n",
    "For the checking script, you will extract `nbest` triples in French, Russian, and English. You will rank these triples by frequency, and then by alphabetical order of the triple using `sorted()`. You will use the French GSD corpus, the Russian SynTagRus corpus, and the English EWT corpus. You will store these triples in the following variables:\n",
    "`freq_triples_fr`, `freq_triples_ru`, `freq_triples_en`. Each variable will contain a list of tuples: `(subject, verb, object), freq)`\n",
    "\n",
    "Here is what you should find:\n",
    "\n",
    "French\n",
    "```\n",
    "freq_triples_fr = [(('il', 'fait', 'partie'), 16),\n",
    "\n",
    " ...]\n",
    " ````\n",
    "\n",
    "And the frequencies:\n",
    "```\n",
    "16\n",
    "7\n",
    "7\n",
    "```\n",
    "\n",
    "Russian:\n",
    "```\n",
    "freq_triples_ru = [(('мы', 'имеем', 'дело'), 6),\n",
    "\n",
    " ...]\n",
    " ````\n",
    "\n",
    "And the frequencies:\n",
    "```\n",
    "6\n",
    "4\n",
    "4\n",
    "```\n",
    "\n",
    "English:\n",
    "```\n",
    "freq_triples_en = [(('you', 'have', 'questions'), 22),\n",
    "\n",
    " ...]\n",
    " ````\n",
    "\n",
    "And the frequencies:\n",
    "```\n",
    "22\n",
    "12\n",
    "7\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [path_fr, path_ru, path_en]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('il', 'fait', 'partie'), 16),\n",
       " (('elle', 'fait', 'partie'), 7),\n",
       " (('il', 'comptait', 'habitants'), 7)]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = read_sentences(path_fr)\n",
    "formatted_corpus = split_rows(sentences, column_names_u)\n",
    "formatted_corpus_dict_fr = convert_to_dict(formatted_corpus)\n",
    "freq_pairs_fr, freq_triples_fr = extract_pairs_and_triples(formatted_corpus_dict_fr, nbest)\n",
    "freq_triples_fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Russian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('мы', 'имеем', 'дело'), 6),\n",
       " (('мы', 'имеем', 'что'), 4),\n",
       " (('мы', 'сделаем', 'все'), 4)]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = read_sentences(path_ru)\n",
    "formatted_corpus = split_rows(sentences, column_names_u)\n",
    "formatted_corpus_dict_ru = convert_to_dict(formatted_corpus)\n",
    "freq_pairs_ru, freq_triples_ru = extract_pairs_and_triples(formatted_corpus_dict_ru, nbest)\n",
    "freq_triples_ru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('you', 'have', 'questions'), 22),\n",
       " (('you', 'think', 'what'), 12),\n",
       " (('i', 'do', 'what'), 7)]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = read_sentences(path_en)\n",
    "formatted_corpus = split_rows(sentences, column_names_u)\n",
    "formatted_corpus_dict_en = convert_to_dict(formatted_corpus)\n",
    "freq_pairs_en, freq_triples_en = extract_pairs_and_triples(formatted_corpus_dict_en, nbest)\n",
    "freq_triples_en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolving the entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now extract the relations involving named entities, that is where both the subject and the object are proper nouns. These proper nouns have a specific part of speech in the `UPOS` column.\n",
    "\n",
    "Write an `extract_entity_triples(formatted_corpus_dict)` that will process the corpus and return a list of `(subject, verb, object)` triples. You will leave the case as it is in the `FORM` column, for instance _United States_ and not _united states_.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entity_triples(formatted_corpus_dict):\n",
    "    triples = []\n",
    "    for i,sentence in enumerate(formatted_corpus_dict):\n",
    "        for word in sentence:\n",
    "            if formatted_corpus_dict[i][word]['DEPREL'].startswith('nsubj') and formatted_corpus_dict[i][word]['UPOS'].startswith('PROPN'):\n",
    "                verb = formatted_corpus_dict[i][word]['HEAD'] #verb connected with nsubj\n",
    "                first = formatted_corpus_dict[i][word]['FORM']\n",
    "                second = formatted_corpus_dict[i][verb]['FORM']\n",
    "                for word2 in sentence: #Iterate again and look for object\n",
    "                    obj = formatted_corpus_dict[i][word2]['DEPREL']\n",
    "                    if obj.startswith('obj') and formatted_corpus_dict[i][word2]['UPOS'].startswith('PROPN'):\n",
    "                        third = formatted_corpus_dict[i][word2]['FORM']\n",
    "                        if verb == formatted_corpus_dict[i][word2]['HEAD']: # pointing at the same verb?\n",
    "                            pair = (first, second, third)\n",
    "                            triples.append(pair)\n",
    "    return triples "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will run the `extract_entity_triples()` function one the English EWT corpus. You will store the list in the `entity_relation_en` variable and you will sort it with `sorted()`. You will keep the **five** first triples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbest = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two first triples are:\n",
    "```\n",
    "[('Baba', 'remember', 'George'),\n",
    " ('Beschta', 'told', 'Planet'),\n",
    "...]\n",
    " ```\n",
    "Note that this time, we keep the original case and the triples are in the alphabetical order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_relation_en = extract_entity_triples(formatted_corpus_dict_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Baba', 'remember', 'George'),\n",
       " ('Beschta', 'told', 'Planet'),\n",
       " ('Boi', 'beat', 'Lopez'),\n",
       " ('Bush', 'mentioned', 'Arabia'),\n",
       " ('Bush', 'mentioned', 'Osama')]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_relation_en = sorted(entity_relation_en)[:nbest]\n",
    "entity_relation_en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting only the headword of the subject and object if often incomplete and uninformative. You can extract all the chunk instead. \n",
    "\n",
    "As an exercise, you will extract manually from the corpus the complete proper nouns in this triple `('Baba', 'remember', 'George')`. You will first consider `Baba` and `George`. You will then look at the verb `remember`.\n",
    "\n",
    "**In your report**, you will give the complete value of the two proper nouns as well as the verb: `(Complete value of the subject, Complete value of the verb, Complete value of the object)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional exercise: Automatic Extraction of the chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an optional exercise, you can implement an automatic baseline technique and extract adjacent proper nouns. You may also want to apply the chunker of the 4th assignment to the corpus to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Optional exercise: Mapping the entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the chunker assignment, you may also want to complement your assignment with a entity solver that will link the entities to wikidata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the article: _PRISMATIC: Inducing Knowledge from a Large Scale Lexicalized Relation Resource_ by Fan and al. (2010) [<a href=\"http://www.aclweb.org/anthology/W/W10/W10-0915.pdf\">pdf</a>] and write in a few sentences how it relates to your work in this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have written all the code and run all the cells, fill in your ID and as well as the name of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "STIL_ID = [\"an8218fr-s\", \"jo2356ri-s\"] # Write your stil ids as a list\n",
    "CURRENT_NOTEBOOK_PATH = os.path.join(os.getcwd(), \n",
    "                                     \"5-triples.ipynb\") # Write the name of your notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The submission code will send your answer. It consists of the pairs and triples in four languages, as well as the triples with named entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"freq_triples_sv\": [[[\"man\", \"v\\\\u00e4nder\", \"sig\"], 14], [[\"det\", \"r\\\\u00f6r\", \"sig\"], 5], [[\"man\", \"s\\\\u00f6ker\", \"arbete\"], 3]], \"freq_triples_fr\": [[[\"il\", \"fait\", \"partie\"], 16], [[\"elle\", \"fait\", \"partie\"], 7], [[\"il\", \"comptait\", \"habitants\"], 7]], \"freq_triples_ru\": [[[\"\\\\u043c\\\\u044b\", \"\\\\u0438\\\\u043c\\\\u0435\\\\u0435\\\\u043c\", \"\\\\u0434\\\\u0435\\\\u043b\\\\u043e\"], 6], [[\"\\\\u043c\\\\u044b\", \"\\\\u0438\\\\u043c\\\\u0435\\\\u0435\\\\u043c\", \"\\\\u0447\\\\u0442\\\\u043e\"], 4], [[\"\\\\u043c\\\\u044b\", \"\\\\u0441\\\\u0434\\\\u0435\\\\u043b\\\\u0430\\\\u0435\\\\u043c\", \"\\\\u0432\\\\u0441\\\\u0435\"], 4]], \"freq_triples_en\": [[[\"you\", \"have\", \"questions\"], 22], [[\"you\", \"think\", \"what\"], 12], [[\"i\", \"do\", \"what\"], 7]], \"entity_relation_en\": [[\"Baba\", \"remember\", \"George\"], [\"Beschta\", \"told\", \"Planet\"], [\"Boi\", \"beat\", \"Lopez\"], [\"Bush\", \"mentioned\", \"Arabia\"], [\"Bush\", \"mentioned\", \"Osama\"]]}'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "ANSWER = json.dumps({'freq_triples_sv': freq_triples_sv,\n",
    "                     'freq_triples_fr': freq_triples_fr,\n",
    "                     'freq_triples_ru': freq_triples_ru,\n",
    "                     'freq_triples_en': freq_triples_en,\n",
    "                     'entity_relation_en': entity_relation_en\n",
    "                    })\n",
    "ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the moment of truth:\n",
    "1. Save your notebook and\n",
    "2. Run the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMISSION_NOTEBOOK_PATH = CURRENT_NOTEBOOK_PATH + \".submission.bz2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "ASSIGNMENT = 5\n",
    "API_KEY = \"f581ba347babfea0b8f2c74a3a6776a7\"\n",
    "\n",
    "# Copy and compress current notebook\n",
    "with bz2.open(SUBMISSION_NOTEBOOK_PATH, mode=\"wb\") as fout:\n",
    "    with open(CURRENT_NOTEBOOK_PATH, \"rb\") as fin:\n",
    "        fout.write(fin.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'msg': None,\n",
       " 'status': 'correct',\n",
       " 'signature': '6f28b40b8f8a155cb0309e937c2b28db3391432da7c2d8ad7af666971eac09e1272b963a8e43866d1f2357dfceb349cd9f8a1ef611954bf5eff7efdaf876c2a3',\n",
       " 'submission_id': '72eebaa2-cb39-469b-9ea1-676aad64b4a9'}"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "res = requests.post(\"https://vilde.cs.lth.se/edan20checker/submit\", \n",
    "                    files={\"notebook_file\": open(SUBMISSION_NOTEBOOK_PATH, \"rb\")}, \n",
    "                    data={\n",
    "                        \"stil_id\": STIL_ID,\n",
    "                        \"assignment\": ASSIGNMENT,\n",
    "                        \"answer\": ANSWER,\n",
    "                        \"api_key\": API_KEY,\n",
    "                    },\n",
    "               verify=True)\n",
    "\n",
    "# from IPython.display import display, JSON\n",
    "res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
